% !TeX program = lualatex

\documentclass[
ngerman,
subtask=ruled %or plain
]{tudaexercise}

\usepackage[english, main=ngerman]{babel}
\usepackage[autostyle]{csquotes}

\usepackage{amsmath,amssymb}

\usepackage{float}

\usepackage{tikz,pgfplots}

\usepackage{biblatex}
%\bibliography{DEMO-TUDaBibliography}

\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\e}{\mathrm{e}}


\ConfigureHeadline{
	headline={HSI Übung 2 - Niklas Beck und Paul Hollmann}
}

\begin{document}
	
	\title[Übung Hochleistungssimulationen]{Hochleistungssimulationen}
	\subtitle{\"Ubung 2}
	\author{Niklas Beck (2582775), Paul Hollmann (2465070)}
	\term{Wintersemester 2023/24}
	%\sheetnumber{1}
	\date{Abgabe 22. Dezember 2023}
	\maketitle
	
	\hrule
	{\Large \textbf{Cannon-Iteration}}
	\hrule
	
	In dieser Übung soll das Matrix-Matrix-Produkt von quadratischen Matrizen per Cannon-Algorithmus untersucht werden.
	Für große Matrizen sind parallele Algorithmen häufig sinnvoll um die Rechenzeit zu kürzen.
	\begin{task}{Matrix-Matrix Multiplikation sequenziell}
			Gegeben seien Matrizen $A = [a_{ij}]$, $B = [b_{ij}]$ und $C = [c_{ij}]$ der Größe $n \times n$.
			Die Berechnung von $C$ gemäß der Gleichung $C = A \times B + C$ kann wie folgt in Summenschreibweise ausgedrückt werden: 
			\[
			c_{ij} = \sum_{k=1}^{n} a_{ik} \cdot b_{kj} + c_{ij}
			\]
			
			Hier wird jedes Element $c_{ij}$ der Matrix $C$ als die Summe der Produkte der entsprechenden Elemente von Zeile $i$ der Matrix $A$ mit Spalte $j$ der Matrix $B$ definiert, zu der dann das entsprechende Element $c_{ij}$ in $C$ addiert wird.
			
			Dies können wir mit drei \texttt{for}-Schleifen realisieren. 
			Dabei speichern wir aber die Matrizen als gepitchte 1D-Arrays und bestimmen jedes mal die analogen Indizes.
		
	\end{task}
	
	
	\begin{task}{Algorithmus der Cannon-Iteration}\label{task:1}
		Zuerst werden zwei Matrizen A und B gefüllt, testweise mit der Zahlenreihe von 1 bis $n \times n$.
		In dieser Übung werden wir doppelte Schleifen zur Füllung, Lesen oder Berechnen mit einer Matrix häufig sehen.
		Also hier bereits eine doppelte Initialisierungsschleife. Gleich danach auch eine einfache Testfunktion um Matrizen auf die Konsole ausgeben zu können.
		
		Ausgelagert in eine Funktion programmieren wir einen Schritt der Cannon-Iteration.
		Die Vertauschung in steigender Anzahl an Tauschvorgängen braucht eine Schleife mehr. Zum Tausch muss eine Zwischenspeicherung verwendet werden.
		
		Anschließend wird der eigentliche Cannon-Iterationsschritt gemacht. So häufig wie die Matrizen Zeilen (oder Spalten) besitzen werden die folgenden beiden Schritte gemacht:
		\begin{itemize}
			\item Berechne die Multiplikation der Elemente $A_{ij}$ und $B_{ij}$ für die Ergebnismatrix an gleicher Position.
			\item Vertausche in Matrix A alle Elemente spaltenweise nach links und in Matrix B alle Elemente zeilenweise nach oben.
		\end{itemize}
		Durch diese Vertauschungen werden alle notwendigen Elemente der Matrizen miteinander multipliziert und es ergibt sich das gewünschte Produkt.
		
		Zur Erweiterung der Berechnung für Blockmatrizen muss die Speicherung zuerst angepasst werden. Ein Zweidimensionales Array dient uns hier als Mittel. Darauf müssen die Funktionsdefinitionen und der Befehlszeilen-Output angepasst werden.
		Auch werden kleine Berechnungen mit Gesamtgröße der Matrix $n$ und Wurzel der Prozessoranzahl $p$ gebraucht.
		In der Funktion zur Cannon-Iteration müssen viele Schleifenköpfe auf die Anzahl der Durchläufe angepasst werden.
		Der größte Unterschied liegt in einer ausformulierte Matrixmultiplikation statt einer einfachen Multiplikation.
	\end{task}
	
	
	\begin{task}{Paralleler Algorithmus mit MPJ}

		Ziel der Veranstaltung ist das parallele Rechnen, heute mit MPI und damit mehreren Prozessen, die sich per Nachrichten verständigen.
		Nach Installation des MPJ, also dem Java Ableger von MPI, ist es nur eine kleine Hürde MPJ in das Programm zu inkludieren und mit mehreren Prozessen zu starten.
		Wichtigste Befehle sind dazu MPI.Init() und zum Ende MPI.finalize(). Die Daten des Prozessor-ranks, der Anzahl an Prozessen und viele Funktionen sind über den Communicator MPI.COMM\_WORLD bestimmt. Z.B. MPI.COMM\_WORLD.Size(); für die Prozessanzahl.
		
		Wir starten den Parallelen Teil indem eine Barrier auf den Host Thread wartet.
		Die Daten der 3 Matrizen wurden im Host bereits Initialisiert und werden anschließend per Scatter verteilt.
		
		Entscheidenster Teil und wichtig zum Lernen ist das Senden von einem Prozess zu einem Anderen im zweiten Teilschritten von Cannon.
		Hierzu muss eine passende Send oder receive Variable erst initialisiert werden. Die Funktionen brauchen zudem den richtigen Rank eines Prozessors fürs Senden und richtige Offsets für die Daten. Die Länge, also die Anzahl eines Datentyps sowie der Typ an sich müssen auch angegeben werden. Also für einen 3 mal 3 Teilblock sollten 9 Floats verschickt werden.
		
		Bevor am Ende jedes Cannon-Schritts alle Matrizen ihre Ergebnisse weitersenden können, muss der Communicator per Barrier alle Prozessen blockieren, sodass gleichzeitig gesendet wird und kein Zwischenergebnis verloren geht.
		
		Der Algorithmus endet mit dem Gather Befehl. Es muss eine komplette Matrix am Host Prozess abgerufen werden können.
	

		Um den Cannon-Algorithmus effizient mit $p \times p$ Prozessen ausführen zu können,
		müssen die Matrixelemente der Matratzen $A$, $B$ und $C$ auf die einzelnen Prozesse aufgeteilt werden.
		Die gesamten Matrizen werden dazu in einen eindimensionalen Array gespeichert. 
		Die Indizierung für den $(x,y)$-ten Eintrag im $(i,j)$-ten Block lautet hierbei
		\begin{equation*}
			M_{xy}^{ij} = {M}_{u}^{\mathrm{ges}}~,~~\text{ wobei } u =jddp +idd + yd +x
		\end{equation*}
		in der Gesamt-Matrix.
		
		Mit der oben genannten Indizierung kann nun einfach vom Host-Prozess jede Teil-Matrix an die anderen propagiert werden,
		dazu wird \texttt{MPI.COMM\_WORLD.Scatter} benutzt um die Einträge an die lokalen Zwischenspeicher zu senden.
		Jeder Prozess verwaltet seine eigenen lokalen Daten in je zwei Einträgen für $A^{\mathrm{lkl}}$ und $B^{\mathrm{lkl}}$ und einen Eintrag für $C^{\mathrm{lkl}}$.
		
		Bei der initialen (und auch allen folgenden) Austauschoperation werden die Daten von $A^{\mathrm{lkl}}$ und $B^{\mathrm{lkl}}$ immer in das grade nicht verwendete Array empfangen und die zu senden aus dem entsprechend anderem.
		Diese Sendoperationen werden alle immer non-blocking ausgeführt, somit wird der Befehl \texttt{Isend} benutzt.
	\end{task}
	
	\begin{task}{Zeitmessung}
		Wir fügen an geeigneten Stellen im Code eine Operation ein, die die aktuelle System-Wallclock-Time ausließt und abspeichert.
		Für die parallelen Anteile sollte dies direkt nach einer synchronisierenden Operation erfolgen.
		Die Differenz der beiden Ausdrücke liefert die vergangene Zeit, diese speichern wir in einer Datei ab.
		
	\end{task}
	
	\begin{task}{Ausführung auf dem Lichtenberg-Hochleistungsrechner} 
		Zur Ausführung auf dem Cluster wird ein Job Script gebraucht. 
		Zunächst wird das Programm mit \texttt{build.sh} erstellt.
		Dieses Script nimmt einige Einstellungen vor, lädt die notwendigen Module, baut das Programm unter Nutzung der Maven Bibliothek.
	
		Das batch Dokument \texttt{run.sh} kann per SLURM in die Processqueue übergeben werden und führt es dann in unterschiedlichen Parameterkonfigurationen aus.
		Die beiden Größen der Matrix werden hierbei als Kommandozeilenparameter übergeben.
	\end{task}

	

	\begin{task}{Ergebnisse}
		
		Für das Beispiel einer $6 \times 6$ Matrix auf einem Prozessgitter von $3 \times 3$ prüfen wir die Ergebnisse des parallelen Algorithmus auf dem Cluster mit der sequenziellen Berechnung.
		Dazu müssen die beiden Ergebnismatrizen auf die gleiche Reihenfolge gebracht werden und im Anschluss können alle Einträge im Rahmen der float-Genauigkeit verglichen werden.
		
		
		Um zu prüfen, wie viel uns die MPJ-Umsetzung als Speedup geliefert hat, messen wir verschiedene Laufzeiten.
		Vorgegeben sind $n$ aus $(n = 250 * p * i | i=1...8)$.
		Wir betrachten dazu \autoref{fig:1}, hier sind für festes $i=1$ die Zeiten und Speedup für verschiedene $p$ angegeben.
		
		\begin{figure}
			\begin{tikzpicture}
				\begin{axis}[
					width=\linewidth,
					height=10cm,
					xlabel={$n$},
					ylabel={Zeit (in s)},
					legend style={at={(0.5,-0.2)}, anchor=north,legend columns=-1},
					%ymode=log, % logarithmische Skala für die y-Achse
					xtick=data,
					xticklabels={
						250,500,750,1000,1250,1500,1750,2000,2250,2500
					},
					ymajorgrids=true,
					enlargelimits=0.15,
					]
					
					% Daten für total_time
					\addplot table [x=n, y=total_time, col sep=space] {
						n p d total_time iteration_time total_time_serial
						250 1 250 75.174053 70.522816 42.850940
						500 2 250 245.688419 241.032588 233.302618 
						750 3 250 405.646907 394.764730 858.676219 
						1000 4 250 494.231617 469.955544 1848.466915 
						1250 5 250 542.761112 512.193297 3953.194378 
						1500 6 250 594.218071 550.962515 6337.510202 
						1750 7 250 795.978417 748.914136 10582.444563 
						2000 8 250 1119.335352 1041.499844 15347.158819 
						2250 9 250 1351.718528 1250.672542 23303.131579 
						2500 10 250 1551.732666 1454.309952 29745.586409 
					};
					
					% Daten für total_time_serial
					\addplot table [x=n, y=total_time_serial, col sep=space] {
						n p d total_time iteration_time total_time_serial
						250 1 250 75.174053 70.522816 42.850940
						500 2 250 245.688419 241.032588 233.302618 
						750 3 250 405.646907 394.764730 858.676219 
						1000 4 250 494.231617 469.955544 1848.466915 
						1250 5 250 542.761112 512.193297 3953.194378 
						1500 6 250 594.218071 550.962515 6337.510202 
						1750 7 250 795.978417 748.914136 10582.444563 
						2000 8 250 1119.335352 1041.499844 15347.158819 
						2250 9 250 1351.718528 1250.672542 23303.131579 
						2500 10 250 1551.732666 1454.309952 29745.586409 
					};
					
					\legend{total\_time\_mpi, total\_time\_serial}
				\end{axis}
			\end{tikzpicture}
			\vspace{1cm}
			\begin{tikzpicture}
				\begin{axis}[
					width=\linewidth,
					height=10cm,
					xlabel={$n$},
					ylabel={$~$},
					legend style={at={(0.5,-0.2)}, anchor=north,legend columns=-1},
					%ymode=log, % logarithmische Skala für die y-Achse
					xtick=data,
					xticklabels={
						250,500,750,1000,1250,1500,1750,2000,2250,2500
					},
					ymajorgrids=true,
					enlargelimits=0.15,
					]
					
					\addplot table [x=n, y=np, col sep=space] {
						n np d total_time iteration_time total_time_serial speedup
						250 1 250 75.174053 70.522816 42.850940 0.569
						500 4 250 245.688419 241.032588 233.302618 0.950
						750 9 250 405.646907 394.764730 858.676219 2.118
						1000 16 250 494.231617 469.955544 1848.466915 3.741
						1250 25 250 542.761112 512.193297 3953.194378 7.282
						1500 36 250 594.218071 550.962515 6337.510202 10.667
						1750 49 250 795.978417 748.914136 10582.444563 13.293
						2000 64 250 1119.335352 1041.499844 15347.158819 13.723
						2250 81 250 1351.718528 1250.672542 23303.131579 17.254
						2500 100 250 1551.732666 1454.309952 29745.586409 19.178
					};
					
					% Daten für total_time
					\addplot table [x=n, y=speedup, col sep=space] {
						n np d total_time iteration_time total_time_serial speedup
						250 1 250 75.174053 70.522816 42.850940 0.569
						500 4 250 245.688419 241.032588 233.302618 0.950
						750 9 250 405.646907 394.764730 858.676219 2.118
						1000 16 250 494.231617 469.955544 1848.466915 3.741
						1250 25 250 542.761112 512.193297 3953.194378 7.282
						1500 36 250 594.218071 550.962515 6337.510202 10.667
						1750 49 250 795.978417 748.914136 10582.444563 13.293
						2000 64 250 1119.335352 1041.499844 15347.158819 13.723
						2250 81 250 1351.718528 1250.672542 23303.131579 17.254
						2500 100 250 1551.732666 1454.309952 29745.586409 19.178
					};
			
					
					\legend{$p^2$,speedup}
				\end{axis}
			\end{tikzpicture}
			\caption{Ausführungszeiten bei verschiedener Matrixgröße $n \times n$ bei fester Blockgröße $d \times d = 250 \times 250$.}
			\label{fig:1}
		\end{figure}
		
		
		
	\end{task}


	
	
\end{document}