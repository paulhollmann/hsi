% !TeX program = lualatex

\documentclass[
ngerman,
subtask=ruled %or plain
]{tudaexercise}

\usepackage[english, main=ngerman]{babel}
\usepackage[autostyle]{csquotes}

\usepackage{amsmath,amssymb}

\usepackage{float}

\usepackage{tikz,pgfplots}

\usepackage{biblatex}
%\bibliography{DEMO-TUDaBibliography}

\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\e}{\mathrm{e}}


\ConfigureHeadline{
	headline={HSI Übung 2 - Niklas Beck und Paul Hollmann}
}

\begin{document}
	
	\title[Übung Hochleistungssimulationen]{Hochleistungssimulationen}
	\subtitle{\"Ubung 2}
	\author{Niklas Beck (2582775), Paul Hollmann (2465070)}
	\term{Wintersemester 2023/24}
	%\sheetnumber{1}
	\date{Abgabe 22. Dezember 2023}
	\maketitle
	
	\hrule
	{\Large \textbf{Cannon-Iteration}}
	\hrule
	
	In dieser Übung soll Matrix-Matrix-Produkt per Cannon-Algorithmus untersucht werden.
	Für große Matrizen sind parallele Algorithmen häufig sinnvoll um die Rechenzeit zu kürzen.
	
	\begin{task}{Sequentieller Algorithmus der Cannon-Iteration}\label{task:1}
	
		!!!!!
		Wir entwickeln zunächst ein sequentielles Programm.
		Nach Eingabe eines Wertes $m$, wird eine quadratische Matrix und ein Vektors mit Zufallswerten gefüllt.  Ziel der Aufgabe ist es die Berechnungszeit des Matrix-Vektor-Produkts zu untersuchen.
		Ohne Vorgabe eines Datentyps haben wir uns für den Typ float entschieden.

		
		Für das Speichern der Eingabe- sowie Ausgabevektoren bzw. der Eingabe Matrix nutzen wir native Java Arrays.
		Wichtig für das Speichern der Matrix nutzen wir ein Array mit Stride von $m$ zwischen einzelnen Zeilen.
		Das $(i,j)$-te Element findet sich demnach an Postion $i\cdot m + j$.
		
		Die eigentliche Berechnung lässt sich mit dann mit zwei for-Schleifen realisieren (siehe Code).
		
	\end{task}
	
	
	\begin{task}{Paralleler Algorithmus mit MPJ}
		!!!!!
		Noch bevor Bearbeitung der nächsten Teilaufgabe haben wir uns die Dokumentationen von OpenCL und JOCL herausgesucht und anhand der Beispiele aus dem moodle-Kurs und von JOCL ein Verständnis zur Einbindung von OpenCL-Code erlangt.
		Dann haben wir, noch auf den eigenen Rechnern, einen Kernel in einer Extradatei ausgelagert, da es eine schönere Formatierung ermöglicht.
		Der ganze Code im Kernel ist ein C/C++ Dialekt, was hier noch wenige Auswirkungen hat, aber nicht vergessen werden sollte.
		Als Vorlage zum Kernel dient das Programm aus Aufgabe \ref{task:1}. Wichtige Ergänzungen im Funktionskopf sind hierbei zuerst einmal die Spezifikation als kernel, die Änderung von Arrays zu Pointern als auch die Angabe des Adressraums der Parameter.
		Sinnvoll in diesem ersten Programm ist der global-Adressbereich, da hier die Variablen von host und device verwendet werden können.
		Die Datentypen sind in diesem Fall noch die gleichen, es gibt aber zum Beispiel nicht einfach einen double Typ. Dieser scheint standardmäßig nicht auf allen Geräten verfügbar zu sein.
		Zum Aufsplitten der Berechnung von einzelnen Spalten wird die ID der jeweiligen Threads per get\_global\_id erhalten.
		Diese Zahl wird als Zeilennummer der Matrix und des Ergebnisses verwendet.
		
		Nach diesem Schritt in OpenCL-Code wird die Einbindung in den Java-Code benötigt.
		Es muss eine platform, properties und damit ein device erstellt werden. Das JOCL Device-Object haben wir für ein paar Testdaten mal untersucht und besonders die maximale Anzahl an work\_groups ist für spätere Ziele noch relevant.
		Ein Context wird für das Programm noch benötigt bevor der Kernel per clCreateProgramWithSource in den JOCL Code eingefügt werden kann.
		Der C++ Code muss gebuilded werden und kann im Anschluss, durch clCreateKernel, als Kernel erstellt werden.
		Eine Command Queue wird erschaffen, um den Kernel später für eine asynchrone Ausführung auf das Device zu schicken.
		Etwas mehr Aufmerksamkeit mussten wir der memory object Erstellung schenken. Buffer wurden für alle Parameter und die Rückgabe erstellt und die Größe der Dateitypen sollte dazu beachtet werden.
		Der Kernel wird mit den Buffer-Objekten und möglichen Konstanten verbunden, die Systemzeit wird einmal gemessen und dann geht es endlich zur Ausführung des Kernels:
		$clEnqueueNDRangeKernel(commandQueue, kernel, 1, null,
		global\_work\_size, local\_work\_size,
		0, null, null);$
		Im Ausführungsbefehl können, wichtig für spätere Ziele, die work\_groups angepasst werden. Zuerst verwenden wir hier local\_work\_size=NULL, da sich OpenCL dann selbst eine geeignete Größe bestimmt.
		%Asynchrones Ausführen ? oder nicht und funktioniert daher System.nanoTime()
		Nach der Ausführung wird der Buffer mit dem Ergebnis gelesen und nochmal die Zeit gemessen.
		Zu guter Letzt, immer wichtig bei der Arbeit direkt mit Speicher, müssen die Objekte freigegeben werden.
		Mit den richtigen Timern können wir schon gleich die Zeiten der Berechnung sequentiell und parallel vergleichen.
		
		Vergleicht man die Ergebnisse zwischen Berechnungen auf den verschiedenen Geräten so stellt man minimale Unterschiede fest. 
		Hier kann es sich verschiedene Gleitkomma Multiplizier- und Addiergenauigkeiten oder -optmierungen auf den verschiedenen Geräten handeln. 
	\end{task}
	
	\begin{task}{Zeitmessung}
		!!!!!
		In dieser Teilaufgabe ist es nun verlangt für einige Problemgrößen die Ausführungszeit auf dem Cluster zu messen.
		
		
		\begin{table}[H]
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				\textbf{Problemgröße} $m$ & \textbf{CPU Zeit} [ms] & \textbf{Kernelzeit} [ms] & \textbf{Komplette Devicezeit} [ms]  \\
				\hline
				1 & 0.003343 & 0.037888 & 0.765934 \\
				10 & 0.007416 & 0.033792 & 0.783118 \\
				1000 & 18.08856 & 0.164864 & 6.884962 \\
				2000 & 16.543831 & 0.376832 & 18.906941 \\
				4000 & 37.728688 & 0.945152 & 67.266515 \\
				8000 & 123.748559 & 1.862656 & 248.687903 \\
				15000 & 409.879089 & 2.832384 & 825.980899 \\
				46340 & 3815.686135 & 35.605504 & 7729.755513 \\
				\hline
			\end{tabular}
			
			
			\caption{Vergleich CPU vs. GPU Ausführungen.}
			\label{tab:data}
		\end{table}
		\begin{figure}[H]
			\begin{tikzpicture}
				\begin{axis}[
					width=\linewidth,
					height=5cm,
					xlabel={Problemgröße},
					ylabel={Zeit [ms]},
					legend style={at={(0.9,-0.2)}, anchor=north},
					ymajorgrids=true,
					xmode=log,
					log basis x={10},
					]
					
					\addplot[
					color=blue,
					mark=*,
					]
					coordinates {
						(1, 0.003343)
						(10, 0.007416)
						(1000, 18.08856)
						(2000, 16.543831)
						(4000, 37.728688)
						(8000, 123.748559)
						(15000, 409.879089)
					};
					\addlegendentry{CPU Zeit}
					
					\addplot[
					color=red,
					mark=square,
					]
					coordinates {
						(1, 0.037888)
						(10, 0.033792)
						(1000, 0.164864)
						(2000, 0.376832)
						(4000, 0.945152)
						(8000, 1.862656)
						(15000, 2.832384)
					};
					\addlegendentry{Kernelzeit}
					
				\end{axis}
			\end{tikzpicture}
			\caption{Daten aus \autoref{tab:data} als Plot.}
		\end{figure}
		
		\autoref{tab:data} stellt für verschiedene Problemgrößen die Rechenzeiten unterteilt in CPU Zeit, Kernelzeit und komplette Devicezeit\footnote{Device Speicherallokation + Kopieren der Arrays hin und zurück sowie Aufruf und Ausführen des Kernels.} dar.
		
		Erkennbar ist, dass die Parallelisierung bis Problemgröße $m=10$, keine Beschleunigung der Multiplikation darstellt.
		Die Erstellung der Threads ist bei $m=10$ noch zu aufwendig, dass es ein Vielfaches der Zeit des sequentiellen Codes braucht.
		Bei höheren Problemgrößen gewinnt der Kernel alleine gegen die sequentielle Funktion, allerdings gibt es Overhead bei der Erstellung der Objekte und dem Kopieren der großen Arrays in den device-memory.
		Für das Problem alleine, scheint sich die Parallelisierung noch nicht zu lohnen.
		Kann aber mit den Daten auf dem Device weitergearbeitet fallen die Kopiervorgänge weitestgehend weg, und dann bietet die einen echten zeitlichen Vorteil.
		
	\end{task}
	
	\begin{task}{Ausführung auf dem Lichtenberg-Hochleistungsrechner} 
		Zur Ausführung auf dem Cluster wird ein Job Script gebraucht. Erneut verwenden wir hier Maven, zum Compilieren und Bauen einer JAR Datei.
		Die beiden Größe der Matrix werden als Kommandozeilenparameter übergeben.
		Das batch Dokument vecmat.sh kann per SLURM in die Processqueue übergeben werden.
		
		!!!!!
		
		Dieses nimmt einige Einstellungen vor, lädt die notwendigen Module, baut das Programm unter Nutzung der Maven Bibliothek und führt es dann in unterschiedlichen Parameterkonfigurationen aus.
	\end{task}

	

	\begin{task} {Erwartete Ergebnisse}
		Für das Beispiel einer 6 $/times$6 Matrix auf einem Prozessorengitter von 3$/times$3 prüfen wir die Ergebnisse des parallelen Algorithmus auf dem Cluster mit der sequenziellen Berechnung.
		
		Um zu prüfen, wie viel uns die MPJ-Umsetzung als Speedup geliefert hat, messen wir verschiedene Laufzeiten.
		Vorgegeben sind n aus $(n = 250 * p * i | i=1...8)$.
		
		Tabelle mit Werten
		
		Nun wollen wir den Speed-up in Abhängigkeit der Prozessoren $p \in (1, 64)$ berechnen.
		
		versch Verteilung der Prozesse auf die verfügbaren Knoten, Proz und Kerne
		
		!!!!!
		Als letztes Ziel wollen wir üben eine gute Anzahl an local-work-groups vorzugeben.
		Für die CL\_DEVICE\_MAX\_WORK\_GROUP\_SIZE erhalten wir auf der NVIDIA V100 $1024$.  
		Wir behalten eine konstante Problemgröße von $m = 10000$.
		Da mit eigener gewählter local\_work\_size (nennen wir sie $l$) die global\_work\_size (nennen wir sie $g$) durch die local\_work\_size teilbar sein sollte,
		muss
		$g = m\mod l == 0 ~~? ~~m : (\lfloor m/l \rfloor + 1) \cdot l$
		gelten. 
		Überschüssige Elemente im Kernel deren globale id $\ge m$ ist werden in diesem ignoriert.
	
		Wir variieren nun local\_work\_size so ergibt sich \autoref{fig:data2}.
		
		\begin{figure}[H]
			\begin{tikzpicture}
				\begin{axis}[
					width=\linewidth,
					height=5cm,
					xlabel={local\_work\_size},
					ylabel={Kernelzeit in [ms]},
					ymajorgrids=true,
					]
					
					\addplot[
					color=blue,
					mark=*,
					]
					coordinates {
						(1, 3.1232)
						(16, 1.444864)
						(32, 1.457152)
						(48, 1.46432)
						(64, 1.476608)
						(80, 1.50224)
						(96, 1.57696)
						(112, 1.696768)
						(128, 1.453056)
						(144, 1.442816)
						(160, 1.496064)
						(176, 1.53088)
						(192, 1.549312)
						(208, 1.570784)
						(224, 1.601536)
						(240, 1.696768)
						(256, 1.788928)
						(272, 1.987584)
						(288, 2.085888)
						(304, 2.149376)
						(320, 2.249696)
						(336, 2.335744)
						(352, 2.43712)
						(368, 2.54768)
						(384, 2.656256)
						(400, 2.825216)
						(416, 2.924544)
						(432, 2.973696)
						(448, 3.076096)
						(464, 3.183616)
						(480, 3.29216)
						(496, 3.407872)
						(512, 3.505152)
						(528, 3.664896)
						(544, 3.769376)
						(560, 3.83488)
						(576, 3.941376)
						(592, 4.048896)
						(608, 4.161536)
						(624, 4.268032)
						(640, 4.383776)
						(656, 4.524)
						(672, 4.62336)
						(688, 4.722688)
						(704, 4.827136)
						(720, 4.939776)
						(736, 5.045248)
						(752, 5.159936)
						(768, 5.266432)
						(784, 5.405696)
						(800, 5.511168)
						(816, 5.61664)
						(832, 5.728224)
						(848, 5.842944)
						(864, 5.955584)
						(880, 6.086656)
						(896, 6.1952)
						(912, 6.33856)
						(928, 6.452224)
						(944, 6.559744)
						(960, 6.680576)
						(976, 6.797312)
						(992, 6.926336)
						(1008, 7.066624)
						(1024, 7.195648)
					};		
				\end{axis}
			\end{tikzpicture}
			\caption{Kernelzeit in Abhängigkeit der local\_work\_size für ein konstantes $m=10000$.  }
			\label{fig:data2}
		\end{figure}
		
		Es ist zu erkennen das für eine möglichst geringe Kernellaufzeit hier eine local\_work\_size zwischen 4 und 160 gewählt werden sollte.
		Diese ergibt sich aus der Hardware und Architektur dieser. 
		Faktoren die dies in Kombination mit obigen beeinflussen können sind z.B. Speicherzugriffe.
		Je nach dem können zu viele oder zu wenige Threads einen Overhead erzeugen in dem die Hardware nicht mit ihrer vollen Kapazität genutzt wird.
	\end{task}


	\appendix
	\section{Appendix}
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			$m$ & local\_ & \textbf{gws} & \textbf{kt} \\
			\hline
			10000 & 1 & 10000 & 3.1232 \\
			10000 & 16 & 10000 & 1.444864 \\
			10000 & 32 & 10016 & 1.457152 \\
			10000 & 48 & 10032 & 1.46432 \\
			10000 & 64 & 10048 & 1.476608 \\
			10000 & 80 & 10000 & 1.50224 \\
			10000 & 96 & 10080 & 1.57696 \\
			10000 & 112 & 10080 & 1.696768 \\
			10000 & 128 & 10112 & 1.453056 \\
			10000 & 144 & 10080 & 1.442816 \\
			10000 & 160 & 10080 & 1.496064 \\
			10000 & 176 & 10032 & 1.53088 \\
			10000 & 192 & 10176 & 1.549312 \\
			10000 & 208 & 10192 & 1.570784 \\
			10000 & 224 & 10080 & 1.601536 \\
			10000 & 240 & 10080 & 1.696768 \\
			10000 & 256 & 10240 & 1.788928 \\
			10000 & 512 & 10240 & 3.505152 \\
			10000 & 800 & 10400 & 5.511168 \\
			10000 & 816 & 10608 & 5.61664 \\
			10000 & 832 & 10816 & 5.728224 \\
			10000 & 848 & 10176 & 5.842944 \\
			10000 & 864 & 10368 & 5.955584 \\
			10000 & 880 & 10560 & 6.086656 \\
			10000 & 896 & 10752 & 6.1952 \\
			10000 & 912 & 10032 & 6.33856 \\
			10000 & 1024 & 10240 & 7.195648 \\
			\hline
		\end{tabular}
		\caption{Daten-Tabelle zu \ref{fig:data2} (Auswahl).}
		\label{tab:data2}
	\end{table}

	
	
\end{document}