% !TeX program = lualatex

\documentclass[
ngerman,
subtask=ruled %or plain
]{tudaexercise}

\usepackage[english, main=ngerman]{babel}
\usepackage[autostyle]{csquotes}

\usepackage{amsmath,amssymb}

\usepackage{float}

\usepackage{tikz,pgfplots}

\usepackage{biblatex}
%\bibliography{DEMO-TUDaBibliography}

\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\e}{\mathrm{e}}


\ConfigureHeadline{
	headline={HSI Übung 2 - Niklas Beck und Paul Hollmann}
}

\begin{document}
	
	\title[Übung Hochleistungssimulationen]{Hochleistungssimulationen}
	\subtitle{\"Ubung 2}
	\author{Niklas Beck (2582775), Paul Hollmann (2465070)}
	\term{Wintersemester 2023/24}
	%\sheetnumber{1}
	\date{Abgabe 22. Dezember 2023}
	\maketitle
	
	\hrule
	{\Large \textbf{Cannon-Iteration}}
	\hrule
	
	In dieser Übung soll das Matrix-Matrix-Produkt von quadratischen Matrizen per Cannon-Algorithmus untersucht werden.
	Für große Matrizen sind parallele Algorithmen häufig sinnvoll um die Rechenzeit zu kürzen.
	
	\begin{task}{Sequentieller Algorithmus der Cannon-Iteration}\label{task:1}
		Ein paralleles Programm starten wir erstmal wieder mit der Entwicklung eines sequentiellen.
		Als kleinen Zwischenschritt haben wir erst ohne Teilblöcke in der Matrix gearbeitet und später die Struktur der Matrix erweitert.
		
		Zuerst werden zwei Matrizen A und B gefüllt, testweise mit der Zahlenreihe von 1 bis $n \times n$.
		In dieser Übung werden wir doppelte Schleifen zur Füllung, Lesen oder Berechnen mit einer Matrix häufig sehen.
		Also hier bereits eine doppelte Initialisierungsschleife. Gleich danach auch eine einfache Testfunktion um Matrizen auf die Konsole ausgeben zu können.
		
		Ausgelagert in eine Funktion programmieren wir einen Schritt der Cannon-Iteration.
		Die Vertauschung in steigender Anzahl an Tauschvorgängen braucht eine Schleife mehr. Zum Tausch muss eine Zwischenspeicherung verwendet werden.
		
		Anschließend wird der eigentliche Cannon-Iterationsschritt gemacht. So häufig wie die Matrizen Zeilen (oder Spalten) besitzen werden die folgenden beiden Schritte gemacht:
		\begin{itemize}
			\item Berechne die Multiplikation der Elemente $A_{ij}$ und $B_{ij}$ für die Ergebnismatrix an gleicher Position.
			\item Vertausche in Matrix A alle Elemente spaltenweise nach links und in Matrix B alle Elemente zeilenweise nach oben.
		\end{itemize}
		Durch diese Vertauschungen werden alle notwendigen Elemente der Matrizen miteinander multipliziert und es ergibt sich das gewünschte Produkt.
		
		Zur Erweiterung der Berechnung für Blockmatrizen muss die Speicherung zuerst angepasst werden. Ein Zweidimensionales Array dient uns hier als Mittel. Darauf müssen die Funktionsdefinitionen und der Befehlszeilen-Output angepasst werden.
		Auch werden kleine Berechnungen mit Gesamtgröße der Matrix $n$ und Wurzel der Prozessoranzahl $p$ gebraucht.
		In der Funktion zur Cannon-Iteration müssen viele Schleifenköpfe auf die Anzahl der Durchläufe angepasst werden.
		Der größte Unterschied liegt in einer ausformulierte Matrixmultiplikation statt einer einfachen Multiplikation.
	\end{task}
	
	
	\begin{task}{Paralleler Algorithmus mit MPJ}
		
		!!!!!
		Noch bevor Bearbeitung der nächsten Teilaufgabe haben wir uns die Dokumentationen von OpenCL und JOCL herausgesucht und anhand der Beispiele aus dem moodle-Kurs und von JOCL ein Verständnis zur Einbindung von OpenCL-Code erlangt.
		Dann haben wir, noch auf den eigenen Rechnern, einen Kernel in einer Extradatei ausgelagert, da es eine schönere Formatierung ermöglicht.
		Der ganze Code im Kernel ist ein C/C++ Dialekt, was hier noch wenige Auswirkungen hat, aber nicht vergessen werden sollte.
		Als Vorlage zum Kernel dient das Programm aus Aufgabe \ref{task:1}. Wichtige Ergänzungen im Funktionskopf sind hierbei zuerst einmal die Spezifikation als kernel, die Änderung von Arrays zu Pointern als auch die Angabe des Adressraums der Parameter.
		Sinnvoll in diesem ersten Programm ist der global-Adressbereich, da hier die Variablen von host und device verwendet werden können.
		Die Datentypen sind in diesem Fall noch die gleichen, es gibt aber zum Beispiel nicht einfach einen double Typ. Dieser scheint standardmäßig nicht auf allen Geräten verfügbar zu sein.
		Zum Aufsplitten der Berechnung von einzelnen Spalten wird die ID der jeweiligen Threads per get\_global\_id erhalten.
		Diese Zahl wird als Zeilennummer der Matrix und des Ergebnisses verwendet.
		
		Nach diesem Schritt in OpenCL-Code wird die Einbindung in den Java-Code benötigt.
		Es muss eine platform, properties und damit ein device erstellt werden. Das JOCL Device-Object haben wir für ein paar Testdaten mal untersucht und besonders die maximale Anzahl an work\_groups ist für spätere Ziele noch relevant.
		Ein Context wird für das Programm noch benötigt bevor der Kernel per clCreateProgramWithSource in den JOCL Code eingefügt werden kann.
		Der C++ Code muss gebuilded werden und kann im Anschluss, durch clCreateKernel, als Kernel erstellt werden.
		Eine Command Queue wird erschaffen, um den Kernel später für eine asynchrone Ausführung auf das Device zu schicken.
		Etwas mehr Aufmerksamkeit mussten wir der memory object Erstellung schenken. Buffer wurden für alle Parameter und die Rückgabe erstellt und die Größe der Dateitypen sollte dazu beachtet werden.
		Der Kernel wird mit den Buffer-Objekten und möglichen Konstanten verbunden, die Systemzeit wird einmal gemessen und dann geht es endlich zur Ausführung des Kernels:
		$clEnqueueNDRangeKernel(commandQueue, kernel, 1, null,
		global\_work\_size, local\_work\_size,
		0, null, null);$
		Im Ausführungsbefehl können, wichtig für spätere Ziele, die work\_groups angepasst werden. Zuerst verwenden wir hier local\_work\_size=NULL, da sich OpenCL dann selbst eine geeignete Größe bestimmt.
		%Asynchrones Ausführen ? oder nicht und funktioniert daher System.nanoTime()
		Nach der Ausführung wird der Buffer mit dem Ergebnis gelesen und nochmal die Zeit gemessen.
		Zu guter Letzt, immer wichtig bei der Arbeit direkt mit Speicher, müssen die Objekte freigegeben werden.
		Mit den richtigen Timern können wir schon gleich die Zeiten der Berechnung sequentiell und parallel vergleichen.
		
		Vergleicht man die Ergebnisse zwischen Berechnungen auf den verschiedenen Geräten so stellt man minimale Unterschiede fest. 
		Hier kann es sich verschiedene Gleitkomma Multiplizier- und Addiergenauigkeiten oder -optmierungen auf den verschiedenen Geräten handeln. 
	\end{task}
	
	\begin{task}{Zeitmessung}
		!!!!!
		In dieser Teilaufgabe ist es nun verlangt für einige Problemgrößen die Ausführungszeit auf dem Cluster zu messen.
		
		
		\begin{table}[H]
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				\textbf{Problemgröße} $m$ & \textbf{CPU Zeit} [ms] & \textbf{Kernelzeit} [ms] & \textbf{Komplette Devicezeit} [ms]  \\
				\hline
				1 & 0.003343 & 0.037888 & 0.765934 \\
				10 & 0.007416 & 0.033792 & 0.783118 \\
				1000 & 18.08856 & 0.164864 & 6.884962 \\
				2000 & 16.543831 & 0.376832 & 18.906941 \\
				4000 & 37.728688 & 0.945152 & 67.266515 \\
				8000 & 123.748559 & 1.862656 & 248.687903 \\
				15000 & 409.879089 & 2.832384 & 825.980899 \\
				46340 & 3815.686135 & 35.605504 & 7729.755513 \\
				\hline
			\end{tabular}
			
			
			\caption{Vergleich CPU vs. GPU Ausführungen.}
			\label{tab:data}
		\end{table}
		\begin{figure}[H]
			\begin{tikzpicture}
				\begin{axis}[
					width=\linewidth,
					height=5cm,
					xlabel={Problemgröße},
					ylabel={Zeit [ms]},
					legend style={at={(0.9,-0.2)}, anchor=north},
					ymajorgrids=true,
					xmode=log,
					log basis x={10},
					]
					
					\addplot[
					color=blue,
					mark=*,
					]
					coordinates {
						(1, 0.003343)
						(10, 0.007416)
						(1000, 18.08856)
						(2000, 16.543831)
						(4000, 37.728688)
						(8000, 123.748559)
						(15000, 409.879089)
					};
					\addlegendentry{CPU Zeit}
					
					\addplot[
					color=red,
					mark=square,
					]
					coordinates {
						(1, 0.037888)
						(10, 0.033792)
						(1000, 0.164864)
						(2000, 0.376832)
						(4000, 0.945152)
						(8000, 1.862656)
						(15000, 2.832384)
					};
					\addlegendentry{Kernelzeit}
					
				\end{axis}
			\end{tikzpicture}
			\caption{Daten aus \autoref{tab:data} als Plot.}
		\end{figure}
		
		\autoref{tab:data} stellt für verschiedene Problemgrößen die Rechenzeiten unterteilt in CPU Zeit, Kernelzeit und komplette Devicezeit\footnote{Device Speicherallokation + Kopieren der Arrays hin und zurück sowie Aufruf und Ausführen des Kernels.} dar.
		
		Erkennbar ist, dass die Parallelisierung bis Problemgröße $m=10$, keine Beschleunigung der Multiplikation darstellt.
		Die Erstellung der Threads ist bei $m=10$ noch zu aufwendig, dass es ein Vielfaches der Zeit des sequentiellen Codes braucht.
		Bei höheren Problemgrößen gewinnt der Kernel alleine gegen die sequentielle Funktion, allerdings gibt es Overhead bei der Erstellung der Objekte und dem Kopieren der großen Arrays in den device-memory.
		Für das Problem alleine, scheint sich die Parallelisierung noch nicht zu lohnen.
		Kann aber mit den Daten auf dem Device weitergearbeitet fallen die Kopiervorgänge weitestgehend weg, und dann bietet die einen echten zeitlichen Vorteil.
		
	\end{task}
	
	\begin{task}{Ausführung auf dem Lichtenberg-Hochleistungsrechner} 
		Zur Ausführung auf dem Cluster wird ein Job Script gebraucht. Erneut verwenden wir hier Maven, zum Compilieren und Bauen einer JAR Datei.
		Die beiden Größe der Matrix werden als Kommandozeilenparameter übergeben.
		Das batch Dokument vecmat.sh kann per SLURM in die Processqueue übergeben werden.
		
		!!!!!
		
		Dieses nimmt einige Einstellungen vor, lädt die notwendigen Module, baut das Programm unter Nutzung der Maven Bibliothek und führt es dann in unterschiedlichen Parameterkonfigurationen aus.
	\end{task}

	

	\begin{task} {Erwartete Ergebnisse}
		Für das Beispiel einer 6 $/times$6 Matrix auf einem Prozessorengitter von 3$/times$3 prüfen wir die Ergebnisse des parallelen Algorithmus auf dem Cluster mit der sequenziellen Berechnung.
		
		Um zu prüfen, wie viel uns die MPJ-Umsetzung als Speedup geliefert hat, messen wir verschiedene Laufzeiten.
		Vorgegeben sind n aus $(n = 250 * p * i | i=1...8)$.
		
		Tabelle mit Werten
		
		Nun wollen wir den Speed-up in Abhängigkeit der Prozessoren $p \in (1, 64)$ berechnen.
		
		versch Verteilung der Prozesse auf die verfügbaren Knoten, Proz und Kerne
	\end{task}


	\appendix
	\section{Appendix}
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			$m$ & local\_ & \textbf{gws} & \textbf{kt} \\
			\hline
			10000 & 1 & 10000 & 3.1232 \\
			10000 & 16 & 10000 & 1.444864 \\
			10000 & 32 & 10016 & 1.457152 \\
			10000 & 48 & 10032 & 1.46432 \\
			10000 & 64 & 10048 & 1.476608 \\
			10000 & 80 & 10000 & 1.50224 \\
			10000 & 96 & 10080 & 1.57696 \\
			10000 & 112 & 10080 & 1.696768 \\
			10000 & 128 & 10112 & 1.453056 \\
			10000 & 144 & 10080 & 1.442816 \\
			10000 & 160 & 10080 & 1.496064 \\
			10000 & 176 & 10032 & 1.53088 \\
			10000 & 192 & 10176 & 1.549312 \\
			10000 & 208 & 10192 & 1.570784 \\
			10000 & 224 & 10080 & 1.601536 \\
			10000 & 240 & 10080 & 1.696768 \\
			10000 & 256 & 10240 & 1.788928 \\
			10000 & 512 & 10240 & 3.505152 \\
			10000 & 800 & 10400 & 5.511168 \\
			10000 & 816 & 10608 & 5.61664 \\
			10000 & 832 & 10816 & 5.728224 \\
			10000 & 848 & 10176 & 5.842944 \\
			10000 & 864 & 10368 & 5.955584 \\
			10000 & 880 & 10560 & 6.086656 \\
			10000 & 896 & 10752 & 6.1952 \\
			10000 & 912 & 10032 & 6.33856 \\
			10000 & 1024 & 10240 & 7.195648 \\
			\hline
		\end{tabular}
		\caption{Daten-Tabelle zu \ref{fig:data2} (Auswahl).}
		\label{tab:data2}
	\end{table}

	
	
\end{document}