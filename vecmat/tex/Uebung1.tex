% !TeX program = lualatex

\documentclass[
ngerman,
subtask=ruled %or plain
]{tudaexercise}

\usepackage[english, main=ngerman]{babel}
\usepackage[autostyle]{csquotes}

\usepackage{amsmath,amssymb}

\usepackage{float}

\usepackage{biblatex}
%\bibliography{DEMO-TUDaBibliography}

\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\e}{\mathrm{e}}


\ConfigureHeadline{
	headline={HSI Übung - Niklas Beck und Paul Hollmann}
}

\begin{document}
	
	\title[Übung Hochleistungssimulationen]{Hochleistungssimulationen}
	\subtitle{\"Ubung 1}
	\author{Niklas Beck (2582775), Paul Hollmann (2465070)}
	\term{Wintersemester 2023/24}
	%\sheetnumber{1}
	\date{Abgabe 26. November 2023}
	\maketitle
	
	\hrule
	{\Large \textbf{Matrix-Vektor-Multiplikation}}
	\hrule
	
	Erklärung als eine Versuchsbeschreibung, Versuchsvorhaben und Arbeitsschritte
	
	\begin{task}{Sequentielle Berechnung des Matrix-Vektor-Produkts}\label{task:1}
		Das Matrix-Vektor-Produkt ist eine wohl bekannte mathematische Operation, welche auch für große Matrizen gebraucht wird.
		Um die Rechenzeit möglichst gering zu halten, sind parallele Berechnungen sinnvoll zu wählen. Insbesondere wenn gleiche Operationen auf unterschiedlichen Daten ausgeführt werden sollen, ist dies ein gutes Einstiegsbeispiel zur Nutzung von OpenCL.
		
		Als sinnvoller erster Schritt brauchen wir ein sequentielles Programm.
		Eine kleine Eigenheit der hier verlangten Funktion ist die Eingabe nur eines Wertes $m$, welcher zur Initialisierung einer quadratischen Matrix und eines Vektors mit Zufallswerten genutzt wird. Wesentliches Ziel der Aufgabe ist es die Berechnungszeit zu untersuchen, weshalb konkrete Beispielmatrizen oder nicht-quadratische Beispiele wenig Mehrwert bieten.
		Ohne Vorgabe eines Datentyps haben wir uns für den Typ float entschieden, da er einen größeren Eingabemenge ermöglicht, aber nicht allzu viel Speicher braucht. Wäre die Präzision relevant, hätten wir uns eher für den double entschieden.
	\end{task}
	
	
	\begin{task}{Parallele Berechnung des Matrix-Vektor-Produkts}
		Noch bevor Bearbeitung der nächsten Teilaufgabe haben wir uns die Dokumentationen von OpenCL und JOCL herausgesucht und anhand der Beispiele aus dem moodle-Kurs und von JOCL ein Verständnis zur Einbindung von OpenCL-Code erlangt.
		Dann haben wir, noch auf den eigenen Rechnern, einen Kernel in einer Extradatei ausgelagert, da es eine schönere Formatierung ermöglicht.
		Der ganze Code im Kernel ist ein C++ Dialekt, was hier noch wenige Auswirkungen hat, aber nicht vergessen werden sollte.
		Als Vorlage zum Kernel dient das Programm aus Aufgabe \ref{task:1}. Wichtige Ergänzungen im Funktionskopf sind hierbei zuerst einmal die Spezifikation als kernel, die Änderung von Arrays zu Pointern als auch die Angabe des Adressraums der Parameter.
		Sinnvoll in diesem ersten Programm ist der global-Adressbereich, da hier die Variablen von host und device verwendet werden können.
		Die Datentypen sind in diesem Fall noch die gleichen, es gibt aber zum Beispiel nicht einfach einen double Typ. Dieser scheint standardmäßig nicht auf allen Geräten verfügbar zu sein.
		Zum Aufsplitten der Berechnung von einzelnen Spalten wird die ID der jeweiligen Threads per get\_global\_id erhalten.
		Diese Zahl wird als Zeilennummer der Matrix und des Ergebnisses verwendet.
		
		Nach diesem Schritt in OpenCL-Code wird die Einbindung in den Java-Code benötigt.
		Es muss eine platform, properties und damit ein device erstellt werden. Das JOCL Device-Object haben wir für ein paar Testdaten mal untersucht und besonders die maximale Anzahl an work\_groups ist für spätere Ziele noch relevant.
		Ein Context wird für das Programm noch benötigt bevor der Kernel per clCreateProgramWithSource in den JOCL Code eingefügt werden kann.
		Der C++ Code muss gebuilded werden und kann im Anschluss, durch clCreateKernel, als Kernel erstellt werden.
		Eine Command Queue wird erschaffen, um den Kernel später für eine asynchrone Ausführung auf das Device zu schicken.
		Etwas mehr Aufmerksamkeit mussten wir der memory object Erstellung schenken. Buffer wurden für alle Parameter und die Rückgabe erstellt und die Größe der Dateitypen sollte dazu beachtet werden.
		Der Kernel wird mit den Buffer-Objekten und möglichen Konstanten verbunden, die Systemzeit wird einmal gemessen und dann geht es endlich zur Ausführung des Kernels:
		$clEnqueueNDRangeKernel(commandQueue, kernel, 1, null,
		global_work_size, n < 0 ? null: local_work_size,
		0, null, null);$
		Im Ausführungsbefehl können, wichtig für spätere Ziele, die work\_groups angepasst werden. Zuerst verwenden wir hier NULL, da sich OpenCL dann selbst eine geeignete Größe bestimmt.
		%Asynchrones Ausführen ? oder nicht und funktioniert daher System.nanoTime()
		Nach der Ausführung wird der Buffer mit dem Ergebnis gelesen und nochmal die Zeit gemessen.
		Zu guter Letzt, immer wichtig bei der Arbeit direkt mit Speicher, müssen die Objekte freigegeben werden.
		Mit den richtigen Timern können wir schon gleich die Zeiten der Berechnung sequentiell und parallel vergleichen.
	\end{task}
	
	
	\begin{task}{Ausführung auf dem Lichtenberg-Hochleistungsrechner} 
		Zur Ausführung auf dem Cluster wird ein Job Script gebraucht. Zur Unterstützung verwenden wir hier Maven.
		Wir erstellen das Dokument vecmat.sh voller batch Befehle.
		Dieses nimmt einige Einstellungen vor, lädt die notwendigen Module, baut das Programm unter Nutzung der Maven Bibliothek und führt es dann in unterschiedlichen Parameterkonfigurationen aus.
	\end{task}

	\begin{task}{Messungen und Analyse} 
		In dieser Teilaufgabe ist es nun verlangt für einige Problemgrößen die Ausführungszeit auf dem Cluster zu messen.
		
		\begin{tabular}{|c|c|c|}
			\hline
			Problemgröße $m$ & sequenzielle Abarbeitung & OpenCL - Parallelisierung \\
			\hline
			10 & 0.008070 ms & 0.788789 ms (0.326362 ms kernel time) \\
			\hline
			1000 & 11.542249 ms & 6.969872 ms (2.329117 ms kernel time) \\
			\hline
			2000 & 16.357809 ms & 18.217549 ms (6.386675 ms kernel time) \\
			\hline
			4000 & 37.825016 ms & 66.511173 ms (22.459137 ms kernel time) \\
			\hline
			8000 & 123.283086 ms & 247.282878 ms (78.444500 ms kernel time) \\
			\hline
			15000 & 409.209827 ms & 827.959536 ms (241.992205 ms kernel time) \\
			\hline
		\end{tabular}
	
		Erkennbar ist, dass die Parallelisierung für diese Problemgröße und ohne Erfahrung noch nicht optimiert, keine Beschleunigung der Multiplikation darstellt.
		Die Erstellung der Threads ist bei $m=10$ noch zu aufwendig, dass es ein Vielfaches der Ausführung des sequentiellen Codes braucht.
		Bei höheren Problemgrößen gewinnt der Kernel alleine gegen die sequentielle Funktion, allerdings gibt es Overhead bei der Erstellung der Objekte und dem Kopieren der großen Arrays in den device-memory.
		Für solch ein kleines Problem, scheint sich die Parallelisierung noch nicht zu lohnen.
	\end{task}

	\begin{task} {Variation der Anzahl der work-groups}
		Variieren Sie manuell die Größe und Anzahl der verwendeten work-groups (siehe oben local\_work\_size) und ermitteln
		Sie jeweils die Dauer der parallelen Abarbeitung bei einer konstanten Problemgröße n (m = 10000). Die Ausführung
		des parallelisierten Programms soll auf dem Lichtenberg-Hochleistungsrechner erfolgen. Dokumentieren Sie die zur
		Ausführung verwendete Hardware und relevante OpenCL Informationen (CL\_DEVICE\_MAX\_WORK\_GROUP\_SIZE etc.).
		Was ist zu beobachten und wie erklärt sich das Ergebnis?
	\end{task}

	
\end{document}